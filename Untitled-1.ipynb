{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/rohinivedam/Documents/Horizon_2.0/EngageVision')\n",
    "from engagevision.head_pose_estimation import HeadPoseEstimator\n",
    "from engagevision.gaze_tracking import GazeTracking\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "from torchvision import models, transforms\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = \"./sam2_repo/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "\n",
    "if not os.path.isfile(checkpoint_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}. \"\n",
    "                            \"Please download it from the official SAM2 repo or Hugging Face.\")\n",
    "\n",
    "else :\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Proceeding with loading the model.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "logging.getLogger(\"hydra\").setLevel(logging.WARNING)\n",
    "\n",
    "# Add the repo root to Python path\n",
    "sys.path.append(\"/Users/rohinivedam/Documents/Horizon_2.0/sam2_repo\")\n",
    "\n",
    "# Import SAM2 model builder and video predictor\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor  # use the correct filename\n",
    "\n",
    "sam = build_sam2(\n",
    "    config_file=config_file,\n",
    "    ckpt_path=checkpoint_path,\n",
    "    device=\"cpu\",            # Force CPU\n",
    "    mode=\"eval\"\n",
    ")\n",
    "\n",
    "def load_hiera_sam_model(config_file, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load a Hierarchical SAM model for video processing.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the configuration file for the SAM model.\n",
    "        checkpoint_path (str): Path to the checkpoint file for the SAM model.\n",
    "\n",
    "    Returns:\n",
    "        SAM2VideoPredictor: An instance of the SAM2VideoPredictor initialized with the\n",
    "        Hierarchical SAM model.\n",
    "    \"\"\"\n",
    "    device = \"cpu\" \n",
    "    sam.to(device)\n",
    "    predictor = SAM2VideoPredictor(sam)  # or SamPredictor(sam) if single-image\n",
    "    return predictor\n",
    "\n",
    "\n",
    "# Example usage\n",
    "config_file = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "checkpoint_path = \"./sam2_repo/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "\n",
    "sam = build_sam2(config_file=config_file, ckpt_path=checkpoint_path)\n",
    "predictor = load_hiera_sam_model(config_file, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc649e17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5574c106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sam2\n",
    "dir(sam2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_frame(predictor, frame):\n",
    "    predictor.set_image(frame)\n",
    "    H, W = frame.shape[:2]\n",
    "    center = np.array([[W // 2, H // 2]])\n",
    "    label = np.array([1])\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=center,\n",
    "        point_labels=label,\n",
    "        multimask_output=False\n",
    "    )\n",
    "    return masks[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18810415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyframes(video_path, predictor, threshold=0.05, stride=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    keyframes = []\n",
    "    prev_mask = None\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % stride != 0:\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "\n",
    "        mask = segment_frame(predictor, frame)\n",
    "\n",
    "        if prev_mask is None or np.mean(np.abs(mask - prev_mask)) > threshold:\n",
    "            keyframes.append(frame)\n",
    "            prev_mask = mask\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b317a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_keyframes(keyframes, size=224):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # mean/std for ImageNet\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return [transform(kf) for kf in keyframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d734b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sequence_length(tensor_list, target_length=32):\n",
    "    seq_len = len(tensor_list)\n",
    "    C, H, W = tensor_list[0].shape\n",
    "\n",
    "    if seq_len >= target_length:\n",
    "        return torch.stack(tensor_list[:target_length])  # [target_length, C, H, W]\n",
    "    else:\n",
    "        pad_count = target_length - seq_len\n",
    "        padding = [tensor_list[-1]] * pad_count\n",
    "        return torch.stack(tensor_list + padding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_pose_estimator = HeadPoseEstimator()\n",
    "gaze_tracker = GazeTracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_engagement_features(frame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract [gaze_x, gaze_y, yaw, pitch, roll] using EngageVision\n",
    "    \"\"\"\n",
    "    gaze_tracker.refresh(frame)\n",
    "    head_pose_estimator.refresh(frame)\n",
    "\n",
    "    # Get gaze direction (x, y)\n",
    "    gaze_x = gaze_tracker.horizontal_ratio() or 0.0\n",
    "    gaze_y = gaze_tracker.vertical_ratio() or 0.0\n",
    "\n",
    "    # Get head pose (yaw, pitch, roll)\n",
    "    pose = head_pose_estimator.get_angles()\n",
    "    if pose is None:\n",
    "        yaw, pitch, roll = 0.0, 0.0, 0.0\n",
    "    else:\n",
    "        yaw, pitch, roll = pose\n",
    "\n",
    "    return np.array([gaze_x, gaze_y, yaw, pitch, roll], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_sequence(video_frames: List[np.ndarray], sequence_length: int = 32) -> torch.Tensor:\n",
    "    features = [extract_engagement_features(f) for f in video_frames]\n",
    "\n",
    "    # Pad or truncate to fixed length\n",
    "    if len(features) >= sequence_length:\n",
    "        features = features[:sequence_length]\n",
    "    else:\n",
    "        last = features[-1] if features else np.zeros(5, dtype=np.float32)\n",
    "        features += [last] * (sequence_length - len(features))\n",
    "\n",
    "    return torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # [1, T, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngagementRNN(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, 1)  # Output engagement score (0-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)  # [B, T, H]\n",
    "        out = self.out(rnn_out[:, -1, :])  # last time step\n",
    "        return torch.sigmoid(out)  # regression score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Simulated Pipeline Execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate reading 40 frames from webcam or video\n",
    "    cap = cv2.VideoCapture(0)  # Change to 'video.mp4' for video file\n",
    "    frames = []\n",
    "    while len(frames) < 40:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    # Extract time-series features\n",
    "    sequence = build_feature_sequence(frames, sequence_length=32)\n",
    "\n",
    "    # Load model\n",
    "    model = EngagementRNN()\n",
    "\n",
    "    # Predict engagement score\n",
    "    score = model(sequence)\n",
    "    print(f\"Predicted Engagement Score: {score.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
